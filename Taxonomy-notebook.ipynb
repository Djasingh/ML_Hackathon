{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eccf2af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 17:20:17.313458: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-28 17:20:18.043342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from transformers import AutoTokenizer, TFBertModel\n",
    "from transformers import AutoTokenizer, BertModel,BertForSequenceClassification\n",
    "from transformers import DistilBertForSequenceClassification,Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026e6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(filename=\"Doceree-HCP_Train.csv\"):\n",
    "    data =  pd.read_csv(filename,encoding='latin-1')\n",
    "    if \"TAXONOMY\" in data.columns:\n",
    "        data = data[~(data[\"TAXONOMY\"].isna())].copy()\n",
    "    data[\"text_info\"] = (data[\"KEYWORDS\"].astype(str)+\" \"+\n",
    "                         data[\"USERCITY\"].astype(str)+\" \"+\n",
    "                         data[\"DEVICETYPE\"].astype(str)+\" \"+\n",
    "                         data[\"CHANNELTYPE\"].astype(str)+\" \"+\n",
    "                         data[\"USERAGENT\"].astype(str)+\" \"+\n",
    "                         data[\"URL\"].astype(str)+\" \"+\n",
    "                         data[\"PLATFORMTYPE\"].astype(str)\n",
    "                        )\n",
    "\n",
    "    #data[\"TAXONOMY\"].astype(str)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76dfba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 512,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93724cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: (32313, 16)\n",
      "test size: (28493, 13)\n"
     ]
    }
   ],
   "source": [
    "train_data = data(\"Doceree-HCP_Train.csv\")\n",
    "test_data  = data(\"Doceree-HCP_Test.csv\")\n",
    "train_data['encoded_label'] = train_data['TAXONOMY'].astype('category').cat.codes\n",
    "\n",
    "print(f\"train size: {train_data.shape}\")\n",
    "print(f\"test size: {test_data.shape}\")\n",
    "\n",
    "train_texts  = list(train_data[\"text_info\"].values)\n",
    "train_labels = [int(i) for i in train_data[\"encoded_label\"].values]\n",
    "\n",
    "train_labels = OneHotEncoder().fit_transform(np.array(train_labels).reshape(-1,1)).toarray()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c0ca27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['encoded_label'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739370d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/dhananjay/Competition/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "train_token_id = []\n",
    "train_attention_masks = []\n",
    "for sample in train_texts:\n",
    "    train_encoding_dict = preprocessing(sample, tokenizer)\n",
    "    train_token_id.append(train_encoding_dict['input_ids']) \n",
    "    train_attention_masks.append(train_encoding_dict['attention_mask'])\n",
    "train_token_id = torch.cat(train_token_id, dim = 0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim = 0)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "\n",
    "val_token_id = []\n",
    "val_attention_masks = []\n",
    "for sample in val_texts:\n",
    "    val_encoding_dict = preprocessing(sample, tokenizer)\n",
    "    val_token_id.append(val_encoding_dict['input_ids']) \n",
    "    val_attention_masks.append(val_encoding_dict['attention_mask'])\n",
    "val_token_id = torch.cat(val_token_id, dim = 0)\n",
    "val_attention_masks = torch.cat(val_attention_masks, dim = 0)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_)\n",
    "# val_encodings   = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef4bcac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb0bc2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HCPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, token_id, attention_masks, labels):\n",
    "        self.token_id = token_id\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item={}\n",
    "        item[\"input_ids\"] = self.token_id[idx]\n",
    "        item[\"attention_mask\"] = self.attention_masks[idx]\n",
    "        item['labels'] = self.labels[idx].to(torch.float32)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = HCPDataset(train_token_id, train_attention_masks, train_labels)\n",
    "val_dataset = HCPDataset(val_token_id, val_attention_masks, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc684a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.encodings.data[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52eb8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba72cce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = train_data['encoded_label'].unique().shape[0]\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels, output_attentions= False, output_hidden_states = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cce7e59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhananjay/Competition/venv/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1616' max='1616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1616/1616 13:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.213400</td>\n",
       "      <td>0.019071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.017985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.017375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1616, training_loss=0.07886931226395144, metrics={'train_runtime': 839.363, 'train_samples_per_second': 30.797, 'train_steps_per_second': 1.925, 'total_flos': 3436801033881600.0, 'train_loss': 0.07886931226395144, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-yelp-polarity\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_taxonomy',          \n",
    "    num_train_epochs=1,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    # strength of weight decay\n",
    "    #logging_dir='./logs1',           \n",
    "    #logging_steps=1000,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end=False,\n",
    "    save_strategy = \"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset             \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d165ea95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [101/101 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.01728697493672371,\n",
       " 'eval_runtime': 51.1213,\n",
       " 'eval_samples_per_second': 126.425,\n",
       " 'eval_steps_per_second': 1.976,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ea54eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"./Taxonomy_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5df0a41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./Taxonomy_model/tokenizer_config.json',\n",
       " './Taxonomy_model/special_tokens_map.json',\n",
       " './Taxonomy_model/vocab.txt',\n",
       " './Taxonomy_model/added_tokens.json',\n",
       " './Taxonomy_model/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df9d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model= DistilBertForSequenceClassification.from_pretrained(save_directory)\n",
    "#tokenizer = DistilBertTokenizerFast.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4720086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e21673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model.to(device)\n",
    "test_texts =list(test_data[\"text_info\"].values)\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# We need Token IDs and Attention Mask for inference on the new sentence\n",
    "test_output = []\n",
    "for i in range(0,len(test_texts),batch_size):\n",
    "    test_ids = []\n",
    "    test_attention_mask = []\n",
    "    for sample in test_texts[i:i+batch_size]:\n",
    "        encoding = preprocessing(sample, tokenizer)\n",
    "        test_ids.append(encoding['input_ids']) \n",
    "        test_attention_mask.append(encoding['attention_mask'])\n",
    "    \n",
    "   \n",
    "    test_ids = torch.cat(test_ids, dim = 0)\n",
    "    test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "\n",
    "    # Forward pass, calculate logit predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(test_ids.to(device),attention_mask = test_attention_mask.to(device))\n",
    "        test_output.append(output.logits.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "422b545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = np.concatenate(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28f3f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_f = np.argmax(test_predict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbe4af0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28493,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predict_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16ee279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2lbl = dict(zip(train_data['encoded_label'], train_data['TAXONOMY']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e87e2f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_taxonomy = [id2lbl[i] for i in test_predict_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38406ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpc_submission = pd.read_csv(\"Doceree-HCP_Submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03faeea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>IS_HCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115501</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115503</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115504</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115505</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  IS_HCP\n",
       "0  115501       0\n",
       "1  115502       1\n",
       "2  115503       0\n",
       "3  115504       0\n",
       "4  115505       1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpc_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4f5b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpc_submission[\"TAXONOMY\"] = test_predict_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b3b92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpc_submission.to_csv(\"Doceree-TAXONOMY_Submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2aea13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>IS_HCP</th>\n",
       "      <th>TAXONOMY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115501</td>\n",
       "      <td>0</td>\n",
       "      <td>2084P0800X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115502</td>\n",
       "      <td>1</td>\n",
       "      <td>2084P0800X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115503</td>\n",
       "      <td>0</td>\n",
       "      <td>2084P0800X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115504</td>\n",
       "      <td>0</td>\n",
       "      <td>2084N0400X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>115505</td>\n",
       "      <td>1</td>\n",
       "      <td>2084P0800X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  IS_HCP    TAXONOMY\n",
       "0  115501       0  2084P0800X\n",
       "1  115502       1  2084P0800X\n",
       "2  115503       0  2084P0800X\n",
       "3  115504       0  2084N0400X\n",
       "4  115505       1  2084P0800X"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpc_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b9bbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
